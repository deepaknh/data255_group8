# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NFQzhDWDE5QUIAs3LXWLgHce1uDAdCSU
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import RandomOverSampler
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

class DataPreprocessor:
    def __init__(self, data):
        self.data = data

    def split_dataset(self, label_column):
        X = self.data['posts']
        y = self.data[label_column]
        return train_test_split(X, y, test_size=0.2, random_state=42)

    def oversample_data(self, X_train, y_train):
        ros = RandomOverSampler(random_state=42)
        if len(X_train.shape) == 1:
            X_train = X_train.values.reshape(-1, 1)

        X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)
        return pd.Series(X_train_resampled.flatten()), y_train_resampled

    def tokenize_and_pad(self, X, tokenizer=None, num_words=10000, maxlen=2000):
        if tokenizer is None:
            tokenizer = Tokenizer(num_words=num_words)
            tokenizer.fit_on_texts(X)
        sequences = tokenizer.texts_to_sequences(X)
        X_padded = pad_sequences(sequences, maxlen=maxlen)
        return X_padded, tokenizer

    def label_encoding(self, y):
        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(y)
        label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
        print("Label Mapping:", label_mapping)
        return y_encoded, label_encoder

